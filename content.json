{"meta":{"title":"开心每一天","subtitle":null,"description":null,"author":null,"url":"http://yoursite.com"},"pages":[],"posts":[{"title":"SpringBoot自动配置原理","slug":"SpringBoot自动配置原理","date":"2018-11-14T09:20:08.000Z","updated":"2018-11-19T07:53:24.000Z","comments":true,"path":"2018/11/14/SpringBoot自动配置原理/","link":"","permalink":"http://yoursite.com/2018/11/14/SpringBoot自动配置原理/","excerpt":"运行原理Spring Boot的运行是由注解@EnableAutoConfiguration提供的。","text":"运行原理Spring Boot的运行是由注解@EnableAutoConfiguration提供的。 12345678910111213@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(&#123;EnableAutoConfigurationImportSelector.class&#125;)public @interface EnableAutoConfiguration &#123; String ENABLED_OVERRIDE_PROPERTY = &quot;spring.boot.enableautoconfiguration&quot;; Class&lt;?&gt;[] exclude() default &#123;&#125;; String[] excludeName() default &#123;&#125;;&#125; 这里的关键功能是@Import注解。EnableAutoConfigurationImportSelector使用SpringFactoriesLoader.loadFactoryNames方法来扫描具有MEAT-INF/spring.factories文件的jar包（1.5版本以前使用EnableAutoConfigurationImportSelector类，1.5以后这个类废弃了使用的是AutoConfigurationImportSelector类），下面是spring-boot-autoconfigure-1.5.4.RELEASE.jar下的MEAT-INF中的spring.factories文件的部分内容。123456789101112131415161718192021222324252627# Initializersorg.springframework.context.ApplicationContextInitializer=\\org.springframework.boot.autoconfigure.SharedMetadataReaderFactoryContextInitializer,\\org.springframework.boot.autoconfigure.logging.AutoConfigurationReportLoggingInitializer# Application Listenersorg.springframework.context.ApplicationListener=\\org.springframework.boot.autoconfigure.BackgroundPreinitializer# Auto Configuration Import Listenersorg.springframework.boot.autoconfigure.AutoConfigurationImportListener=\\org.springframework.boot.autoconfigure.condition.ConditionEvaluationReportAutoConfigurationImportListener# Auto Configuration Import Filtersorg.springframework.boot.autoconfigure.AutoConfigurationImportFilter=\\org.springframework.boot.autoconfigure.condition.OnClassCondition# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\ 里面的类都是自动配置类，SpringBoot会根据这些自动配置类去自动配置环境。下面我们就自动动手写一个starter。 自定义Starter首先先介绍几个条件注解。 @ConditionalOnBean：当容器里有指定的Bean为true @ConditionalOnClass：当类路径下有指定的类为true @ConditionalOnMissingBean：当容器里没有指定的Bean为true @ConditionalOnProperty：指定的数据是否有指定的值 了解了条件注解后，我们开始自定义Starter。 在自定义Starter之前先要在Maven中填写依赖。 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.miaolovezhen&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spring-boot-starter-test&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.6.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;version&gt;1.5.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 完成TestProperties类，这个类定义了默认的属性值，如该类中，只有一个属性值msg，默认为world。@ConfigurationProperties注解会定义一个匹配，如果想修改属性值，可以在application.properties中使用“匹配.属性=修改的值”进行修改。如test.msg = tan。 12345678910111213@ConfigurationProperties(prefix = &quot;test&quot;)public class TestProperties &#123; private static final String MSG = &quot;springboot&quot;; private String msg = MSG; public String getMsg() &#123; return msg; &#125; public void setMsg(String msg) &#123; this.msg = msg; &#125;&#125; 完成服务类。服务类是指主要的功能类，如果没有SpringBoot，这些服务类在Spring中都是需要自己去配置生成的。如SpringMVC中的DispatcherServlet、Mybatis的DataSource等。 123456789101112131415public class TestService &#123; private String msg; public String sayHello()&#123; return &quot;Hello &quot; + msg; &#125; public String getMsg() &#123; return msg; &#125; public void setMsg(String msg) &#123; this.msg = msg; &#125;&#125; 完成自动配置类。自动配置类主要作用是SpringBoot的配置核心，它会写在MEAT-INF/spring.factories中，告知SpringBoot在启动时去读取该类并根据该类的规则进行配置。 @EnableConfigurationProperties注解根据TestProperties类开启属性注入，允许在application.properties修改里面的属性值。 @ConditionOnClass会检测是否存在TestService类。 @ConditionOnProperty类会查看是否开启该自动配置。默认开启（true）。 12345678910111213141516@Configuration@EnableConfigurationProperties(TestProperties.class)@ConditionalOnClass(TestService.class)@ConditionalOnProperty(prefix = &quot;test&quot; , value = &quot;enabled&quot; , matchIfMissing = true)public class TestServiceAutoConfiguration &#123; @Autowired TestProperties testProperties; @Bean @ConditionalOnMissingBean(TestService.class) public TestService testService()&#123; TestService testService = new TestService(); testService.setMsg(testProperties.getMsg()); return testService; &#125;&#125; 最后一步，不要忘记在在MEAT-INF文件夹中创建spring.factories文件。内容很简单，告诉SpringBoot去读取TestServiceAutoConfiguration类。 12org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\cn.miaolovezhen.TestServiceAutoConfiguration 好啦，搞定！下面可以使用maven install命令把starter存到本地，其他SpringBoot项目需要使用这个starter，直接导入就可以啦。","categories":[],"tags":[{"name":"spring boot","slug":"spring-boot","permalink":"http://yoursite.com/tags/spring-boot/"}]},{"title":"nginx事件模块介绍","slug":"nginx事件模块介绍","date":"2018-11-14T08:42:27.000Z","updated":"2018-11-19T07:48:41.000Z","comments":true,"path":"2018/11/14/nginx事件模块介绍/","link":"","permalink":"http://yoursite.com/2018/11/14/nginx事件模块介绍/","excerpt":"事件模块的初始化众所周知，nginx是master/worker框架，在nginx启动时是一个进程，在启动的过程中master会fork出了多个子进程作为worker。master主要是管理worker，本身并不处理请求。而worker负责处理请求。因此，事件模块的初始化也是分成两部分。一部分发生在fork出worker前，主要是配置文件解析等操作，另外一部分发生在fork之后，主要是向epoll中添加监听事件。","text":"事件模块的初始化众所周知，nginx是master/worker框架，在nginx启动时是一个进程，在启动的过程中master会fork出了多个子进程作为worker。master主要是管理worker，本身并不处理请求。而worker负责处理请求。因此，事件模块的初始化也是分成两部分。一部分发生在fork出worker前，主要是配置文件解析等操作，另外一部分发生在fork之后，主要是向epoll中添加监听事件。 启动进程对事件模块的初始化启动进程对事件模块的初始化分为配置文件解析、开始监听端口和ngx_event_core_module模块的初始化。这三个步骤均在ngx_init_cycle函数进行。 调用关系：main() —&gt; ngx_init_cycle() 下图是ngx_init_cycle函数的流程，红框是本节将要介绍的三部分内容。 配置文件解析启动进程的一个主要工作是解析配置文件。在nginx中，用户主要通过nginx配置文件nginx.conf的event块来控制和调节事件模块的参数。下面是一个event块配置的示例：123456789101112131415user nobody;worker_processes 1;error_log logs/error.log;pid logs/nginx.pid;...... events &#123; use epoll; worker_connections 1024; accept_mutex on；&#125; http &#123; ......&#125; 首先我们先看看nginx是如何解析event块，并将event块存储在什么地方。 在nginx中，解析配置文件的工作是调用ngx_init_cycle函数完成的。下图是该函数在解析配置文件部分的一个流程： ngx_init_cycle函数首先会进行一些初始化工作，包括更新时间，创建内存池和创建并更新ngx_cycle_t结构体cycle； 调用各个core模块的create_conf方法，可以创建cycle的conf_ctx数组，该阶段完成后cycle-&gt;conf_ctx如下图所示： 初始化ngx_conf_t类型的结构体conf，将cycle-&gt;conf_ctx结构体赋值给conf的ctx字段 解析配置文件。解析配置文件会调用ngx_conf_parse函数，该函数会解析一行命令，当遇到块时会递归调用自身。解析的方法也很简单，就是读取一个命令，然后在所有模块的cmd数组中寻找该命令，若找到则调用该命令的cmd-&gt;set()，完成参数的解析。event命令是在event/ngx_event.c文件中定义的，代码如下。1234567891011static ngx_command_t ngx_events_commands[] = &#123; &#123; ngx_string(&quot;events&quot;), NGX_MAIN_CONF|NGX_CONF_BLOCK|NGX_CONF_NOARGS, ngx_events_block, 0, 0, NULL &#125;, ngx_null_command&#125;; 在从配置文件中读取到event后，会调用ngx_events_block函数。下面是ngx_events_block函数的主要工作： 解析完配置文件中的event块后，cycle-&gt;conf_ctx如下图所示： 解析完整个配置文件后，调用各个core类型模块的init_conf方法。ngx_event_module的ctx的init_conf方法为ngx_event_init_conf。该方法并没有实际的用途，暂不详述。 监听socket虽然监听socket和事件模块并没有太多的关系，但是为了使得整个流程完整，此处会简单介绍一下启动进程是如何监听端口的。 该过程首先检查old_cycle，如果old_cycle中有和cycle中相同的socket，就直接把old_cycle中的fd赋值给cycle。之后会调用ngx_open_listening_socket函数，监听端口。 下面是ngx_open_listening_sockets函数，该函数的作用是遍历所有需要监听的端口，然后调用socket()，bind()和listen()函数，该函数会重试5次。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465ngx_int_tngx_open_listening_sockets(ngx_cycle_t *cycle)&#123; ...... /* 重试5次 */ for (tries = 5; tries; tries--) &#123; failed = 0; /* 遍历需要监听的端口 */ ls = cycle-&gt;listening.elts; for (i = 0; i &lt; cycle-&gt;listening.nelts; i++) &#123; ...... /* ngx_socket函数就是socket函数 */ s = ngx_socket(ls[i].sockaddr-&gt;sa_family, ls[i].type, 0); ...... /* 设置socket属性 */ if (setsockopt(s, SOL_SOCKET, SO_REUSEADDR, (const void *) &amp;reuseaddr, sizeof(int)) == -1) &#123; ...... &#125; ...... /* IOCP事件操作 */ if (!(ngx_event_flags &amp; NGX_USE_IOCP_EVENT)) &#123; if (ngx_nonblocking(s) == -1) &#123; ...... &#125; &#125; ...... /* 绑定socket和地址 */ if (bind(s, ls[i].sockaddr, ls[i].socklen) == -1) &#123; ...... &#125; ...... /* 开始监听 */ if (listen(s, ls[i].backlog) == -1) &#123; ...... &#125; ls[i].listen = 1; ls[i].fd = s; &#125; ...... /* 两次重试间隔500ms */ ngx_msleep(500); &#125; ...... return NGX_OK;&#125; ngx_event_core_module模块的初始化在ngx_init_cycle函数监听完端口，并提交新的cycle后，便会调用ngx_init_modules函数，该方法会遍历所有模块并调用其init_module方法。对于该阶段，和事件驱动模块有关系的只有ngx_event_core_module的ngx_event_module_init方法。该方法主要做了下面三个工作： 获取core模块配置结构体中的时间精度timer_resolution，用在epoll里更新缓存时间 调用getrlimit方法，检查连接数是否超过系统的资源限制 利用 mmap 分配一块共享内存，存储负载均衡锁（ngx_accept_mutex）、连接计数器（ngx_connection_counter） worker进程对事件模块的初始化启动进程在完成一系列操作后，会fork出master进程，并自我关闭，让master进程继续完成初始化工作。master进程会在ngx_spawn_process函数中fork出worker进程，并让worker进程调用ngx_worker_process_cycle函数。ngx_worker_process_cycle函数是worker进程的主循环函数，该函数首先会调用ngx_worker_process_init函数完成worker的初始化，然后就会进入到一个循环中，持续监听处理请求。 事件模块的初始化就发生在ngx_worker_process_init函数中。 其调用关系：main() —&gt; ngx_master_process_cycle() —&gt; ngx_start_worker_processes() —&gt; ngx_spawn_process() —&gt; ngx_worker_process_cycle() —&gt; ngx_worker_process_init()。 对于ngx_worker_process_init函数，会调用各个模块的init_process方法： 1234567891011121314151617static voidngx_worker_process_init(ngx_cycle_t *cycle, ngx_int_t worker)&#123; ...... for (i = 0; cycle-&gt;modules[i]; i++) &#123; if (cycle-&gt;modules[i]-&gt;init_process) &#123; if (cycle-&gt;modules[i]-&gt;init_process(cycle) == NGX_ERROR) &#123; /* fatal */ exit(2); &#125; &#125; &#125; ......&#125; 在此处，会调用ngx_event_core_module的ngx_event_process_init函数。该函数较为关键，将会重点解析。在介绍ngx_event_process_init函数前，先介绍两个终于的结构体，由于这两个结构体较为复杂，故只介绍部分字段： ngx_event_s结构体。nginx中，事件会使用ngx_event_s结构体来表示。 1234567891011121314151617181920212223242526272829303132333435363738394041424344struct ngx_event_s &#123; /* 通常指向ngx_connection_t结构体 */ void *data; /* 事件可写 */ unsigned write:1; /* 事件可建立新连接 */ unsigned accept:1; /* 检测事件是否过期 */ unsigned instance:1; /* 通常将事件加入到epoll中会将该字段置为1 */ unsigned active:1; ...... /* 事件超时 */ unsigned timedout:1; /* 事件是否在定时器中 */ unsigned timer_set:1; ...... /* 事件是否在延迟处理队列中 */ unsigned posted:1; ...... /* 事件的处理函数 */ ngx_event_handler_pt handler; ...... /* 定时器红黑树节点 */ ngx_rbtree_node_t timer; /* 延迟处理队列节点 */ ngx_queue_t queue; ......&#125;; ngx_connection_s结构体代表一个nginx连接 1234567891011121314151617181920struct ngx_connection_s &#123; /* 若该结构体未使用，则指向下一个为使用的ngx_connection_s，若已使用，则指向ngx_http_request_t */ void *data; /* 指向一个读事件结构体，这个读事件结构体表示该连接的读事件 */ ngx_event_t *read; /* 指向一个写事件结构体，这个写事件结构体表示该连接的写事件 */ ngx_event_t *write; /* 连接的套接字 */ ngx_socket_t fd; ...... /* 该连接对应的监听端口，表示是由该端口建立的连接 */ ngx_listening_t *listening; ......&#125;; 下面介绍ngx_event_process_init函数的实现，代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135/* 此方法在worker进程初始化时调用 */static ngx_int_tngx_event_process_init(ngx_cycle_t *cycle)&#123; ...... /* 打开accept_mutex负载均衡锁，用于防止惊群 */ if (ccf-&gt;master &amp;&amp; ccf-&gt;worker_processes &gt; 1 &amp;&amp; ecf-&gt;accept_mutex) &#123; ngx_use_accept_mutex = 1; ngx_accept_mutex_held = 0; ngx_accept_mutex_delay = ecf-&gt;accept_mutex_delay; &#125; else &#123; ngx_use_accept_mutex = 0; &#125; /* 初始化两个队列，一个用于存放不能及时处理的建立连接事件，一个用于存储不能及时处理的读写事件 */ ngx_queue_init(&amp;ngx_posted_accept_events); ngx_queue_init(&amp;ngx_posted_events); /* 初始化定时器 */ if (ngx_event_timer_init(cycle-&gt;log) == NGX_ERROR) &#123; return NGX_ERROR; &#125; /** * 调用使用的ngx_epoll_module的ctx的actions的init方法，即ngx_epoll_init函数 * 该函数主要的作用是调用epoll_create()和创建用于epoll_wait()返回事件链表的event_list **/ for (m = 0; cycle-&gt;modules[m]; m++) &#123; ...... if (module-&gt;actions.init(cycle, ngx_timer_resolution) != NGX_OK) &#123; exit(2); &#125; break; &#125; /* 如果在配置中设置了timer_resolution，则要设置控制时间精度。通过setitimer方法会设置一个定时器，每隔timer_resolution的时间会发送一个SIGALRM信号 */ if (ngx_timer_resolution &amp;&amp; !(ngx_event_flags &amp; NGX_USE_TIMER_EVENT)) &#123; ...... sa.sa_handler = ngx_timer_signal_handler; sigemptyset(&amp;sa.sa_mask); if (sigaction(SIGALRM, &amp;sa, NULL) == -1) &#123; ...... &#125; itv.it_interval.tv_sec = ngx_timer_resolution / 1000; ...... if (setitimer(ITIMER_REAL, &amp;itv, NULL) == -1) &#123; ...... &#125; &#125; ...... /* 分配连接池空间 */ cycle-&gt;connections = ngx_alloc(sizeof(ngx_connection_t) * cycle-&gt;connection_n, cycle-&gt;log); ...... c = cycle-&gt;connections; /* 分配读事件结构体数组空间，并初始化读事件的closed和instance */ cycle-&gt;read_events = ngx_alloc(sizeof(ngx_event_t) * cycle-&gt;connection_n, cycle-&gt;log); ...... rev = cycle-&gt;read_events; for (i = 0; i &lt; cycle-&gt;connection_n; i++) &#123; rev[i].closed = 1; rev[i].instance = 1; &#125; /* 分配写事件结构体数组空间，并初始化写事件的closed */ cycle-&gt;write_events = ngx_alloc(sizeof(ngx_event_t) * cycle-&gt;connection_n, cycle-&gt;log); ...... wev = cycle-&gt;write_events; for (i = 0; i &lt; cycle-&gt;connection_n; i++) &#123; wev[i].closed = 1; &#125; /* 将序号为i的读事件结构体和写事件结构体赋值给序号为i的connections结构体的元素 */ i = cycle-&gt;connection_n; next = NULL; do &#123; i--; /* 将connection的data字段设置为下一个connection */ c[i].data = next; c[i].read = &amp;cycle-&gt;read_events[i]; c[i].write = &amp;cycle-&gt;write_events[i]; c[i].fd = (ngx_socket_t) -1; next = &amp;c[i]; &#125; while (i); /* 初始化cycle-&gt;free_connections */ cycle-&gt;free_connections = next; cycle-&gt;free_connection_n = cycle-&gt;connection_n; /* 为每个监听端口分配连接 */ ls = cycle-&gt;listening.elts; for (i = 0; i &lt; cycle-&gt;listening.nelts; i++) &#123; ...... c = ngx_get_connection(ls[i].fd, cycle-&gt;log); ...... rev = c-&gt;read; ...... /* 为监听的端口的connection结构体的read事件设置回调函数 */ rev-&gt;handler = (c-&gt;type == SOCK_STREAM) ? ngx_event_accept : ngx_event_recvmsg; /* 将监听的connection的read事件添加到事件驱动模块（epoll） */ ...... if (ngx_add_event(rev, NGX_READ_EVENT, 0) == NGX_ERROR) &#123; return NGX_ERROR; &#125; &#125; return NGX_OK;&#125; 该方法主要做了下面几件事： 打开accept_mutex负载均衡锁，用于防止惊群。惊群是指当多个worker都处于等待事件状态，如果突然来了一个请求，就会同时唤醒多个worker，但是只有一个worker会处理该请求，这就造成系统资源浪费。为了解决这个问题，nginx使用了accept_mutex负载均衡锁。各个worker首先会抢锁，抢到锁的worker才会监听各个端口。 初始化两个队列，一个用于存放不能及时处理的建立连接事件，一个用于存储不能及时处理的读写事件。 初始化定时器，该定时器就是一颗红黑树，根据时间对事件进行排序。 调用使用的ngx_epoll_module的ctx的actions的init方法，即ngx_epoll_init函数。该函数较为简单，主要的作用是调用epoll_create()和创建用于存储epoll_wait()返回事件的链表event_list。 如果再配置中设置了timer_resolution，则要设置控制时间精度，用于控制nginx时间。这部分在后面重点讲解。 分配连接池空间、读事件结构体数组、写事件结构体数组。上文介绍了ngx_connection_s和ngx_event_s结构体，我们了解到每一个ngx_connection_s结构体都有两个ngx_event_s结构体，一个读事件，一个写事件。在这个阶段，会向内存池中申请三个数组：cycle-&gt;connections、cycle-&gt;read_events和cycle-&gt;write_events，并将序号为i的读事件结构体和写事件结构体赋值给序号为i的connections结构体的元素。并将cycle-&gt;free_connections指向第一个未使用的ngx_connections结构体。 为每个监听端口分配连接。在此阶段，会获取cycle-&gt;listening数组中的ngx_listening_s结构体元素。在前面，我们已经讲了nginx启动进程会监听端口，并将socket连接的fd存储在cycle-&gt;listening数组中。在这里，会获取监听的端口，并为每个监听分配连接结构体。 为每个监听端口的连接的读事件设置handler。在为cycle-&gt;listening的元素分配完ngx_connection_s类型的连接后，会为连接的读事件设置回调方法handler。这里handler为ngx_event_accept函数，对于该函数，将在后文讲解。 将每个监听端口的连接的读事件添加到epoll中。在此处，会调用ngx_epoll_module的ngx_epoll_add_event函数，将监听端口的连接的读事件(ls[i].connection-&gt;read)添加到epoll中。ngx_epoll_add_event函数的流程如下： 在向epoll中添加事件前，需要判断之前是否添加过该连接的事件。至此，ngx_event_process_init的工作完成，事件模块的初始化也完成了。后面worker开始进入循环监听阶段。 事件处理worker的主循环函数ngx_worker_process_cycleworker在初始化完成之后，开始循环监听端口，并处理请求。下面开始我们开始讲解worker是如何处理事件的。worker的循环代码如下：12345678910111213141516171819202122232425262728293031323334353637static voidngx_worker_process_cycle(ngx_cycle_t *cycle, void *data)&#123; ngx_int_t worker = (intptr_t) data; ngx_process = NGX_PROCESS_WORKER; ngx_worker = worker; /* 初始化worker */ ngx_worker_process_init(cycle, worker); ngx_setproctitle(&quot;worker process&quot;); for ( ;; ) &#123; if (ngx_exiting) &#123; ...... &#125; ngx_log_debug0(NGX_LOG_DEBUG_EVENT, cycle-&gt;log, 0, &quot;worker cycle&quot;); /* 处理IO事件和时间事件 */ ngx_process_events_and_timers(cycle); if (ngx_terminate) &#123; ...... &#125; if (ngx_quit) &#123; ...... &#125; if (ngx_reopen) &#123; ...... &#125; &#125;&#125; 可以看到，在worker初始化后进入一个for循环，所有的IO事件和时间事件都是在函数ngx_process_events_and_timers中处理的。 worker的事件处理函数ngx_process_events_and_timers在worker的主循环中，所有的事件都是通过函数ngx_process_events_and_timers处理的，该函数的代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/* 事件处理函数和定时器处理函数 */voidngx_process_events_and_timers(ngx_cycle_t *cycle)&#123; ngx_uint_t flags; ngx_msec_t timer, delta; /* timer_resolution模式，设置epoll_wait函数阻塞ngx_timer_resolution的时间 */ if (ngx_timer_resolution) &#123; /* timer_resolution模式 */ timer = NGX_TIMER_INFINITE; flags = 0; &#125; else &#123; /* 非timer_resolution模式，epoll_wait函数等待至下一个定时器事件到来时返回 */ timer = ngx_event_find_timer(); flags = NGX_UPDATE_TIME; &#125; /* 是否使用accept_mutex */ if (ngx_use_accept_mutex) &#123; /** * 该worker是否负载过高，若负载过高则不抢锁 * 判断负载过高是判断该worker建立的连接数是否大于该worker可以建立的最大连接数的7/8 **/ if (ngx_accept_disabled &gt; 0) &#123; ngx_accept_disabled--; &#125; else &#123; /* 抢锁 */ if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) &#123; return; &#125; if (ngx_accept_mutex_held) &#123; /* 抢到锁，则收到事件后暂不处理，先扔到事件队列中 */ flags |= NGX_POST_EVENTS; &#125; else &#123; /* 未抢到锁，要修改worker在epoll_wait函数等待的时间，使其不要过大 */ if (timer == NGX_TIMER_INFINITE || timer &gt; ngx_accept_mutex_delay) &#123; timer = ngx_accept_mutex_delay; &#125; &#125; &#125; &#125; /* delta用于计算ngx_process_events的耗时 */ delta = ngx_current_msec; /* 事件处理函数，epoll使用的是ngx_epoll_process_events函数 */ (void) ngx_process_events(cycle, timer, flags); delta = ngx_current_msec - delta; ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle-&gt;log, 0, &quot;timer delta: %M&quot;, delta); /* 处理ngx_posted_accept_events队列的连接事件 */ ngx_event_process_posted(cycle, &amp;ngx_posted_accept_events); /* 若持有accept_mutex，则释放锁 */ if (ngx_accept_mutex_held) &#123; ngx_shmtx_unlock(&amp;ngx_accept_mutex); &#125; /* 若事件处理函数的执行时间不为0，则要处理定时器事件 */ if (delta) &#123; ngx_event_expire_timers(); &#125; /* 处理ngx_posted_events队列的读写事件 */ ngx_event_process_posted(cycle, &amp;ngx_posted_events);&#125; ngx_process_events_and_timers函数是nginx处理事件的核心函数，主要的工作可以分为下面几部分： 设置nginx更新时间的方式。 nginx会将时间存储在内存中，每隔一段时间调用ngx_time_update函数更新时间。那么多久更新一次呢？nginx提供两种方式： 方式一：timer_resolution模式。在nginx配置文件中，可以使用timer_resolution之类来选择此方式。如果使用此方式，会将epoll_wait的阻塞时间设置为无穷大，即一直阻塞。那么如果nginx一直都没有收到事件，会一直阻塞吗？答案是不会的。在上文中讲解的ngx_event_process_init函数(第5步)将会设置一个时间定时器和一个信号处理函数，其中时间定时器会每隔timer_resolution的时间发送一个SIGALRM信号，而当worker收到时间定时器发送的信号，会将epoll_wait函数终端，同时调用SIGALRM信号的中断处理函数，将全局变量ngx_event_timer_alarm置为1。后面会检查该变量，调用ngx_time_update函数来更新nginx的时间。 方式二：如果不在配置文件中设置timer_resolution，nginx默认会使用方式二来更新nginx的时间。首先会调用ngx_event_find_timer函数来设置epoll_wait的阻塞时间，ngx_event_find_timer函数返回的是下一个时间事件发生的时间与当前时间的差值，即让epoll_wait阻塞到下一个时间事件发生为止。当使用这种模式，每当epoll_wait返回，都会调用ngx_time_update函数更新时间。 使用负载均衡锁ngx_use_accept_mutex。 上文曾经提过一个问题，当多个worker都处于等待事件状态，如果突然来了一个请求，就会同时唤醒多个worker，但是只有一个worker会处理该请求，这就造成系统资源浪费。nginx如果解决这个问题呢？答案就是使用一个锁来解决。在监听事件前，各个worker会进行一次抢锁行为，只有抢到锁的worker才会监听端口，而其他worker值处理已经建立连接的事件。 首先函数会通过ngx_accept_disabled是否大于0来判断是否过载，过载的worker是不允许抢锁的。ngx_accept_disabled的计算方式如下。12345/** * ngx_cycle-&gt;connection_n是每个进程最大连接数，也是连接池的总连接数，ngx_cycle-&gt;free_connection_n是连接池中未使用的连接数量。 * 当未使用的数量小于总数量的1/8时，会使ngx_accept_disabled大于0。这时认为该worker过载。 **/ngx_accept_disabled = ngx_cycle-&gt;connection_n / 8 - ngx_cycle-&gt;free_connection_n; 若ngx_accept_disabled小于0，worker可以抢锁。这时会通过ngx_trylock_accept_mutex函数抢锁。该函数的流程如下图所示： 在抢锁结束后，若worker抢到锁，设置该worker的flag为NGX_POST_EVENTS，表示抢到锁的这个worker在收到事件后并不会立即调用事件的处理函数，而是会把事件放到一个队列里，后期处理。 调用事件处理函数ngx_process_events，epoll使用的是ngx_epoll_process_events函数。此代码较为重要，下面是代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153static ngx_int_tngx_epoll_process_events(ngx_cycle_t *cycle, ngx_msec_t timer, ngx_uint_t flags)&#123; int events; uint32_t revents; ngx_int_t instance, i; ngx_uint_t level; ngx_err_t err; ngx_event_t *rev, *wev; ngx_queue_t *queue; ngx_connection_t *c; ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle-&gt;log, 0, &quot;epoll timer: %M&quot;, timer); /* 调用epoll_wait，从epoll中获取发生的事件 */ events = epoll_wait(ep, event_list, (int) nevents, timer); err = (events == -1) ? ngx_errno : 0; /* 两种方式更新nginx时间，timer_resolution模式ngx_event_timer_alarm为1，非timer_resolution模式flags &amp; NGX_UPDATE_TIME不为0，均会进入if条件 */ if (flags &amp; NGX_UPDATE_TIME || ngx_event_timer_alarm) &#123; ngx_time_update(); &#125; /* 处理epoll_wait返回为-1的情况 */ if (err) &#123; /** * 对于timer_resolution模式，如果worker接收到SIGALRM信号，会调用该信号的处理函数，将ngx_event_timer_alarm置为1，从而更新时间。 * 同时如果在epoll_wait阻塞的过程中接收到SIGALRM信号，会中断epoll_wait，使其返回NGX_EINTR。由于上一步已经更新了时间，这里要把ngx_event_timer_alarm置为0。 **/ if (err == NGX_EINTR) &#123; if (ngx_event_timer_alarm) &#123; ngx_event_timer_alarm = 0; return NGX_OK; &#125; level = NGX_LOG_INFO; &#125; else &#123; level = NGX_LOG_ALERT; &#125; ngx_log_error(level, cycle-&gt;log, err, &quot;epoll_wait() failed&quot;); return NGX_ERROR; &#125; /* 若events返回为0，判断是因为epoll_wait超时还是其他原因 */ if (events == 0) &#123; if (timer != NGX_TIMER_INFINITE) &#123; return NGX_OK; &#125; ngx_log_error(NGX_LOG_ALERT, cycle-&gt;log, 0, &quot;epoll_wait() returned no events without timeout&quot;); return NGX_ERROR; &#125; /* 对epoll_wait返回的链表进行遍历 */ for (i = 0; i &lt; events; i++) &#123; c = event_list[i].data.ptr; /* 从data中获取connection &amp; instance的值，并解析出instance和connection */ instance = (uintptr_t) c &amp; 1; c = (ngx_connection_t *) ((uintptr_t) c &amp; (uintptr_t) ~1); /* 取出connection的read事件 */ rev = c-&gt;read; /* 判断读事件是否过期 */ if (c-&gt;fd == -1 || rev-&gt;instance != instance) &#123; ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle-&gt;log, 0, &quot;epoll: stale event %p&quot;, c); continue; &#125; /* 取出事件的类型 */ revents = event_list[i].events; ngx_log_debug3(NGX_LOG_DEBUG_EVENT, cycle-&gt;log, 0, &quot;epoll: fd:%d ev:%04XD d:%p&quot;, c-&gt;fd, revents, event_list[i].data.ptr); /* 若连接发生错误，则将EPOLLIN、EPOLLOUT添加到revents中，在调用读写事件时能够处理连接的错误 */ if (revents &amp; (EPOLLERR|EPOLLHUP)) &#123; ngx_log_debug2(NGX_LOG_DEBUG_EVENT, cycle-&gt;log, 0, &quot;epoll_wait() error on fd:%d ev:%04XD&quot;, c-&gt;fd, revents); revents |= EPOLLIN|EPOLLOUT; &#125; /* 事件为读事件且读事件在epoll中 */ if ((revents &amp; EPOLLIN) &amp;&amp; rev-&gt;active) &#123; #if (NGX_HAVE_EPOLLRDHUP) if (revents &amp; EPOLLRDHUP) &#123; rev-&gt;pending_eof = 1; &#125; rev-&gt;available = 1;#endif rev-&gt;ready = 1; /* 事件是否需要延迟处理？对于抢到锁监听端口的worker，会将事件延迟处理 */ if (flags &amp; NGX_POST_EVENTS) &#123; /* 根据事件的是否是accept事件，加到不同的队列中 */ queue = rev-&gt;accept ? &amp;ngx_posted_accept_events : &amp;ngx_posted_events; ngx_post_event(rev, queue); &#125; else &#123; /* 若不需要延迟处理，直接调用read事件的handler */ rev-&gt;handler(rev); &#125; &#125; /* 取出connection的write事件 */ wev = c-&gt;write; /* 事件为写事件且写事件在epoll中 */ if ((revents &amp; EPOLLOUT) &amp;&amp; wev-&gt;active) &#123; /* 判断写事件是否过期 */ if (c-&gt;fd == -1 || wev-&gt;instance != instance) &#123; ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle-&gt;log, 0, &quot;epoll: stale event %p&quot;, c); continue; &#125; wev-&gt;ready = 1;#if (NGX_THREADS) wev-&gt;complete = 1;#endif /* 事件是否需要延迟处理？对于抢到锁监听端口的worker，会将事件延迟处理 */ if (flags &amp; NGX_POST_EVENTS) &#123; ngx_post_event(wev, &amp;ngx_posted_events); &#125; else &#123; /* 若不需要延迟处理，直接调用write事件的handler */ wev-&gt;handler(wev); &#125; &#125; &#125; return NGX_OK;&#125; 该函数的流程图如下： 计算ngx_process_events函数的调用时间。 处理ngx_posted_accept_events队列的连接事件。这里就是遍历ngx_posted_accept_events队列，调用事件的handler方法，这里accept事件的handler为ngx_event_accept。 释放负载均衡锁。 处理定时器事件，具体操作是在定时器红黑树中查找过期的事件，调用其handler方法。 处理ngx_posted_events队列的读写事件，即遍历ngx_posted_events队列，调用事件的handler方法。 至此，我们介绍完了nginx事件模块的事件处理函数ngx_process_events_and_timers。nginx事件模块的相关知识也初步介绍完了。","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"事件模块命令源码解析","slug":"事件模块命令源码解析","date":"2018-11-14T06:26:03.000Z","updated":"2018-11-19T07:48:59.000Z","comments":true,"path":"2018/11/14/事件模块命令源码解析/","link":"","permalink":"http://yoursite.com/2018/11/14/事件模块命令源码解析/","excerpt":"对于nginx事件模块的介绍，已经在前面的blog中介绍过了。这里我们介绍一下配置文件中与事件模块有关的重要命令和它们的具体实现。下面是一个配置的例子。","text":"对于nginx事件模块的介绍，已经在前面的blog中介绍过了。这里我们介绍一下配置文件中与事件模块有关的重要命令和它们的具体实现。下面是一个配置的例子。123456events&#123; worker_connections 1024; use epoll; multi_accept on; accept_mutex on;｝ worker_connections指令worker_connections命令控制每个worker能够建立的最大连接数。123456789static ngx_command_t ngx_event_core_commands[] = &#123; &#123; ngx_string(&quot;worker_connections&quot;), NGX_EVENT_CONF|NGX_CONF_TAKE1, ngx_event_connections, 0, 0, NULL &#125;, ......&#125; 上面的代码是该命令的定义，可以看出，该命令的解析函数是ngx_event_connections函数。下面是ngx_event_connections函数的核心代码：1234567891011121314static char *ngx_event_connections(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)&#123; ...... //获取worker_connections命令后面的参数 value = cf-&gt;args-&gt;elts; //将参数赋值给ecf-&gt;connections ecf-&gt;connections = ngx_atoi(value[1].data, value[1].len); ...... //将ecf-&gt;connections赋值给cf-&gt;cycle-&gt;connection_n cf-&gt;cycle-&gt;connection_n = ecf-&gt;connections; return NGX_CONF_OK;&#125; 可以看到，我们在worker_connections命令后面写的参数被赋值给了cf-&gt;cycle-&gt;connection_n这个变量。那么cf-&gt;cycle-&gt;connection_n是干什么用的呢？我们看下下面这段代码。123456789101112static ngx_int_tngx_event_process_init(ngx_cycle_t *cycle)&#123; ...... /* 分配连接池空间 */ cycle-&gt;connections = ngx_alloc(sizeof(ngx_connection_t) * cycle-&gt;connection_n, cycle-&gt;log); if (cycle-&gt;connections == NULL) &#123; return NGX_ERROR; &#125; ......&#125; ngx_event_process_init函数是在每个worker初始化的时候调用的。在该函数中，会为每个worker分配一个连接池。worker每建立一个新连接，都需要从连接池中获取一个ngx_connection_t结构体来记录连接的信息，连接断开后会把该结构体释放，重新放回到worker的连接池中。可以看到，这里开辟了一个sizeof(ngx_connection_t) * cycle-&gt;connection_n大小的空间，即每个worker最多同时建立cf-&gt;cycle-&gt;connection_n个连接。这也是使用worker_connections命令控制worker连接数的原理。 use指令use命令可以选择所使用的时间驱动模块。该命令的定义如下：12345678910static ngx_command_t ngx_event_core_commands[] = &#123; ...... &#123; ngx_string(&quot;use&quot;), NGX_EVENT_CONF|NGX_CONF_TAKE1, ngx_event_use, 0, 0, NULL &#125;, ......&#125; 在nginx源码中，使用ngx_event_use函数解析use命令。123456789101112131415161718192021222324static char *ngx_event_use(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)&#123; ...... //遍历所有事件模块 for (m = 0; cf-&gt;cycle-&gt;modules[m]; m++) &#123; if (cf-&gt;cycle-&gt;modules[m]-&gt;type != NGX_EVENT_MODULE) &#123; continue; &#125; //取出事件模块的信息 module = cf-&gt;cycle-&gt;modules[m]-&gt;ctx; //将事件模块的名字与use后面的参数相比较 if (module-&gt;name-&gt;len == value[1].len) &#123; if (ngx_strcmp(module-&gt;name-&gt;data, value[1].data) == 0) &#123; //若相同，则赋值给ecf-&gt;use ecf-&gt;use = cf-&gt;cycle-&gt;modules[m]-&gt;ctx_index; ecf-&gt;name = module-&gt;name-&gt;data; ...... &#125; &#125; return NGX_CONF_OK; &#125; ......&#125; 上面是该函数的核心代码，该函数会遍历所有的NGX_EVENT_MODULE类型的模块，将use命令后面的参数与这些模块的模块名比较，如果相同，就会将该模块的ctx_index赋值给ecf-&gt;use变量。在后面初始化worker的时候，会根据ecf-&gt;use选择相应的事件驱动模块进行初始化。12345678910111213141516171819202122232425static ngx_int_tngx_event_process_init(ngx_cycle_t *cycle)&#123; ...... /* 调用要使用的event模块的ctx的action的init方法 */ for (m = 0; cycle-&gt;modules[m]; m++) &#123; if (cycle-&gt;modules[m]-&gt;type != NGX_EVENT_MODULE) &#123; continue; &#125; if (cycle-&gt;modules[m]-&gt;ctx_index != ecf-&gt;use) &#123; continue; &#125; module = cycle-&gt;modules[m]-&gt;ctx; if (module-&gt;actions.init(cycle, ngx_timer_resolution) != NGX_OK) &#123; /* fatal */ exit(2); &#125; break; &#125; ......&#125; 这里会根据ecf-&gt;use调用要使用的事件模块的ctx的action的init方法，如果我们使用的是epoll模块，则这里会调用ngx_epoll_init函数，为worker创建epoll结构体。 这里还有一个问题，如果我们没有使用use命令指定我们要使用的事件驱动模块，那nginx如何选择事件模块呢？首先，在安装nginx运行./configure命令的时候，脚本已经进行了一次筛选，选出操作系统支持的模块，只对这些模块进行编译。那如果操作系统支持多个模块呢？在ngx_event_core_init_conf函数中，会检查是否使用use指定了事件模块，如果没有则会根据优先级选择一个最优的模块。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364static char *ngx_event_core_init_conf(ngx_cycle_t *cycle, void *conf)&#123; ...... #if (NGX_HAVE_EPOLL) &amp;&amp; !(NGX_TEST_BUILD_EPOLL) //测试是否支持epoll，如果支持让module为ngx_epoll_module fd = epoll_create(100); if (fd != -1) &#123; (void) close(fd); module = &amp;ngx_epoll_module; &#125; else if (ngx_errno != NGX_ENOSYS) &#123; module = &amp;ngx_epoll_module; &#125;#endif#if (NGX_HAVE_DEVPOLL) &amp;&amp; !(NGX_TEST_BUILD_DEVPOLL) //如果支持devpoll，让module等于ngx_devpoll_module module = &amp;ngx_devpoll_module;#endif#if (NGX_HAVE_KQUEUE) //如果支持kqueue，让module等于ngx_kqueue_module module = &amp;ngx_kqueue_module;#endif#if (NGX_HAVE_SELECT) //如果前面三者都不支持，让module等于ngx_select_module if (module == NULL) &#123; module = &amp;ngx_select_module; &#125;#endif //最后，如果前面四个都不支持，就选择modules数组中第一个事件模块 if (module == NULL) &#123; for (i = 0; cycle-&gt;modules[i]; i++) &#123; if (cycle-&gt;modules[i]-&gt;type != NGX_EVENT_MODULE) &#123; continue; &#125; event_module = cycle-&gt;modules[i]-&gt;ctx; if (ngx_strcmp(event_module-&gt;name-&gt;data, event_core_name.data) == 0) &#123; continue; &#125; module = cycle-&gt;modules[i]; break; &#125; &#125; ......&#125; 根据上面的代码，默认的事件模块的优先级为：kqueue &gt; devpoll &gt; epoll &gt; select &gt; 其他模块。 multi_accept指令我们知道在网络编程中，服务器在三次握手之后，需要调用accept函数分配一个新的套接字和客户端进行通信。multi_accept命令可以控制worker在收到accept事件的时候调用多次accept函数，与多个客户端建立通信。12345678910static ngx_command_t ngx_event_core_commands[] = &#123; ...... &#123; ngx_string(&quot;multi_accept&quot;), NGX_EVENT_CONF|NGX_CONF_FLAG, ngx_conf_set_flag_slot, 0, offsetof(ngx_event_conf_t, multi_accept), NULL &#125;, ......&#125; ngx_conf_set_flag_slot函数是nginx实现的一个解析配置的公用函数，该函数可以读取配置，将开关flag(on/off)保存在ngx_event_conf_t结构体中的multi_accept字段中。 nginx在收到accept事件的时候，都会调用ngx_event_accept函数。在该函数中，会使用ecf-&gt;multi_accept字段，控制worker调用accept函数的次数。1234567891011121314151617181920212223voidngx_event_accept(ngx_event_t *ev)&#123; ...... // 获取ngx_event_core_module模块的配置项参数结构 ecf = ngx_event_get_conf(ngx_cycle-&gt;conf_ctx, ngx_event_core_module); //将ecf-&gt;multi_accept赋值给ev-&gt;available if (!(ngx_event_flags &amp; NGX_USE_KQUEUE_EVENT)) &#123; ev-&gt;available = ecf-&gt;multi_accept; &#125; ...... do &#123; ...... // accept 建立一个新的连接 s = accept(lc-&gt;fd, &amp;sa.sockaddr, &amp;socklen); ...... //根据ev-&gt;available判断是否继续循环 &#125; while (ev-&gt;available);&#125; 由于一些事件模块如epoll只知道哪些端口有accept事件，并不知道该端口有多少accept事件，因此在worker调用epoll_wait函数获取到accept事件后，如果设置multi_accept为off，则只会与第一个发起请求的客户端进行accept，而其他的客户端的连接会在下一次调用epoll_wait函数时再处理。如果设置multi_accept为on，则会进入上面的do{......}while()循环中，会一直调用accept函数直到其返回-1为止，即一次便处理该监听端口的所有请求。 accept_mutex指令当多个worker都处于监听端口状态，如果突然来了一个请求，就会同时唤醒多个worker，但是只有一个worker会处理该请求，这就造成系统资源浪费，这个现象称作“惊群”。为了解决这个问题，nginx引用了一个锁。各个worker首先会进行抢锁，抢到锁的worker才会进行监听端口。 我们首先看一下accept_mutex指令的解析。12345678910static ngx_command_t ngx_event_core_commands[] = &#123; ...... &#123; ngx_string(&quot;accept_mutex&quot;), NGX_EVENT_CONF|NGX_CONF_FLAG, ngx_conf_set_flag_slot, 0, offsetof(ngx_event_conf_t, accept_mutex), NULL &#125;, ......&#125; 和上面的multi_accept命令一样，accept_mutex也会调用ngx_conf_set_flag_slot函数进行配置解析，将解析处理的参数存储在ngx_event_conf_t结构体中的accept_mutex字段中。 在worker初始化的过程中，会调用ngx_event_process_init函数，判断是否使用防止惊群的锁。123456789101112131415161718static ngx_int_tngx_event_process_init(ngx_cycle_t *cycle)&#123; ...... //如果nginx是多进程模式，worker数大于1，且ecf-&gt;accept_mutex为1，则打开锁 if (ccf-&gt;master &amp;&amp; ccf-&gt;worker_processes &gt; 1 &amp;&amp; ecf-&gt;accept_mutex) &#123; ngx_use_accept_mutex = 1; ngx_accept_mutex_held = 0; ngx_accept_mutex_delay = ecf-&gt;accept_mutex_delay; &#125; else &#123; //否则不使用锁 ngx_use_accept_mutex = 0; &#125; ......&#125; 从上面的代码可以看出，如果设置accept_mutex为on，全局变量ngx_use_accept_mutex在worker初始化的时候会设置为1，若设置为off则ngx_use_accept_mutex会被设置为0。 后面nginx会在ngx_process_events_and_timers函数处理nginx事件的时候使用ngx_use_accept_mutex变量，控制是否使用锁。123456789101112131415161718192021222324252627282930313233voidngx_process_events_and_timers(ngx_cycle_t *cycle)&#123; ...... // 是否使用accept_mutex if (ngx_use_accept_mutex) &#123; // 该worker是否负载过高，若负载过高则不抢锁 if (ngx_accept_disabled &gt; 0) &#123; ngx_accept_disabled--; &#125; else &#123; // 进行抢锁 if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) &#123; return; &#125; if (ngx_accept_mutex_held) &#123; // 抢到锁 flags |= NGX_POST_EVENTS; &#125; else &#123; // 未抢到锁，要修改worker在epoll_wait函数等待的时间 if (timer == NGX_TIMER_INFINITE || timer &gt; ngx_accept_mutex_delay) &#123; timer = ngx_accept_mutex_delay; &#125; &#125; &#125; &#125; ......&#125;","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]}]}